# Домашнее задание по Hive

* Deadline: 10.08.2023, 23:59

## Введение 
* В этом задании 5 задач, последняя из которых не содержит тестов. Это значит что в ветке sberhivetask5 можно не обращать внимание на "красный" pipeline.
* В отличие от ДЗ по MapReduce, логи Hive перенаправлять в `/dev/null` не нужно.
* В остальном принцип сдачи ДЗ такой же как и в MapReduce.

## Исходные данные: логи пользователей

Данные находятся в HDFS по адресу `/data/user_logs/*_M`. Они состоят из трёх частей, каждая из которых находится в своей поддиректории. Данные в каждой части отличаются количеством и типом колонок, разделенных знаками табуляции ('\t') или пробелами.

#### А. Логи запросов пользователей к новостным сообщениям (user_logs).
1. Ip-адрес, с которого пришел запрос (STRING),
2. Время запроса (TIMESTAMP или INT),
3. Пришедший с ip-адреса http-запрос (STRING),
4. Размер переданной клиенту страницы (INT),
5. Http-статус код (INT).
6. Информация о клиентском приложении, с которого осуществлялся запрос на сервер, в том числе, информация о браузере (STRING).

**Важно:** информация о браузере содержится в начале 6-ого поля лога (символы с нулевой позиции до позиции первого пробельного символа), содержание оставшейся части строки не определяет браузер пользователя. Разделитель между IP и временем запроса имеет 3 табуляции.

#### B. Информация о пользователях (user_data).
1. IP-адрес (STRING),
2. Браузер пользователя (STRING),
3. Пол (STRING) //male, female,
4. Возраст (INT).

#### С. Информация о местонахождении IP адресов пользователей (ip_data).
1. IP-адрес (STRING),
2. Регион (STRING).

## Подготовка кода для сдачи
Для каждой домашки имеются предваритиельно созданные репозитории с веткой master и ветками для каждой задачи в домашке. Для сдачи домашки нужно:

1. В каждой ветке создать директорию, имя директории = имени ветки. Например, нужно создать директории с названиями `sberhivetask1` и `sberhivetask2` в ветках `sberhivetask1` и `sberhivetask2` соотвественно.

2. В созданных директориях положите файл `run.sh`. Это точка входа в вашу программу и именно её будет запускать система проверки. В `run.sh` может быть как всё решение задачи так и вызов других файлов. В данном случае в `run.sh` будет примерно такой код: `hive -f query.sql`.

3. Логи и вывод, в отличие от предыдущей домашки, перенаправлять в /dev/null не нужно.


## Задачи

**Задача 1 (411)**. Создайте внешние (EXTERNAL) таблицы по исходным данным. В результате будет 4 таблицы: логи пользователей, данные ip адресов, данные пользователей и подсети. Из таблицы логов перенесите данные в другую таблицу, партицированную по датам – одна партиция на каждый день. На партиционированных таблицах и нужно будет выполнять запросы в следующих задачах.

Требуется, чтобы сериализация и десериализация данных осуществлялась с использованием регулярных выражений (см. `org.apache.hadoop.hive.serde2.RegexSerDe`).

Проверить правильность создания таблиц с помощью простейших запросов (`SELECT * FROM <table> LIMIT 10`). Эти Select запросы нужно также добавлять в скрипт задачи.

**Дополнительные требования:**
1. Для каждого из вас в системе зарезервирована база, имя которой совпадает с логином на **gitlab2.atp-fivt.org** (например, **ivchenkoon**). Поэтому когда заливаете код на тестировани, пишите `USE <логин_на_gitlab2.atp-fivt.org>`. Для отладки создайте свою базу с любым именем.
2. Не добавляйте код создания базы в GitLab-репозиторий т.к. базы для вас уже созданы и система тестирования не имеет прав на перезаписть этих баз.
3. Таблицы должны называться так:
    * Logs - партиционированная таблица с логами.
    * Users - таблица с информацией о пользователях.
    * IPRegions - таблица с IP и регионами.
    * Subnets - таблица с подсетями (`/data/subnets/variant1`).

*Пример результата:*
```
33.49.147.163	http://lenta.ru/4303000	1189	451	Chrome/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0)n	20140101
75.208.40.166	http://newsru.com/3330815	60	306	Safari/5.0 (Windows; U; MSIE 9.0; Windows NT 8.1; Trident/5.0; .NET4.0E; en-AU)n	20140101
```

**Задача 2.**. Напишите запрос, выбирающий количество посещений для каждого дня. Полученные результаты отсортируйте по убыванию количества.

*Пример результата:*
```
20140308	96
20140409	96
20140318	96
```
Т.к. после агрегации данных становится немного, `LIMIT` использовать не надо.

**Задача 3.**. Напишите запрос, выбирающий количество посещений от мужчин и от женщин по регионам.

*Пример результата:*
```
Tver	66968157	29097223
Voronezh	60445347	26333509
```
Подсказка для задачи 3: используйте конструкцию [IF](https://www.folkstalk.com/2011/11/conditional-functions-in-hive.html).

**Задача 4**. Представьте ситуацию, что все новостные сайты переехали в домен .com. Вас попросили обновить базу логов, чтоб логи пользователей указывали не на старые домены, а на новые. Например, новостная ссылка http://news.rambler.ru/8744806 теперь должна выглядеть в ваших запросах как http://news.rambler.com/8744806. Используйте стриминг в hive-sql запросе. (Рекомендуется обратить внимание на команды awk и sed). Выведите TOP-10 записей логов без сортировки.

*Пример результата:*
```
49.203.96.67	20140102	http://lenta.com/2296722	716	499	Safari/5.0
33.49.147.163	20140102	http://news.yandex.com/5605690	850	300	Safari/5.0
```
Задача 4 должна быть решена с использованием Hive Streaming. Заметьте, что в выводе требуется не только то поле, которое мы изменяем, а все 6 полей.

**Задача 5**. Представьте, что у Вас очень много данных, и требуется обработать эти данные в кратчайший срок. Напишите запрос из предыдущей задачи 3, но с использованием семплирования ([TABLESAMPLE](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Sampling)). Попробуйте различные значения процента семплов от общего объема и сравните точность получаемых оценок в зависимости от процента семплов. Результат отобразите на графике. График тоже закоммитьте в ветку с заданием.

Для оценки точности можно использовать любую метрику, удобную для вас. Для сравнения можно брать как срадние значения по регионам, так и отдельно взятые регионы.

Для этой задачи автоматических тестов нет, поэтому не обращайте внимания на падающие CI-job'ы.

Литература
1. Tom White. Hadoop: The Definitive Guide, 3rd edition. O’Reilly, 2012, глава 12.
2. Edward Capriolo, Dean Wampler, Jason Rutherglen. Programming Hive. O’Reilly, 2012.
3. [Примеры UDTF](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inTable-GeneratingFunctions%28UDTF%29) из Hive wiki.

Ниже приведены баллы, которые вы получаете за каждую задачу.

|Задача | Ветка в репозитории | Баллы
|:--|:--|:--|
|1|sberhivetask1|0.3|
|2|sberhivetask2|0.2|
|3|sberhivetask3|0.3|
|4|sberhivetask4|0.2|
|5|sberhivetask5|0,5|

### Комментарии
1. В некоторых задачах требуется вывести TOP-N записей. Это означает просто использование LIMIT N. Сортировать результат запроса при этом не обязательно. Сортировка нужна только в задачах 2.х.
2. Пожалуйста, отправляйте на тестирование сначала снача 1-ю задачу, а потом все остальные.
3. Тестирование остальных задач происходит на таблицах, которые были созданы в 1-й задаче. Если вы неправильно сделали задачу 1, все остальные тоже не протестируются. 
4. В некоторых ситуациях ваша личная база данных Hive, на которой происходит тестирование, может быть испорчена вашими же запросами. В таком случае, заполните [**форму**](https://forms.gle/MmaPNj9NGvqpNYYr6) для её пересоздания. Это можно сделать в чате курса.
5. Если Hive падает с ошибкой:

```java
ERROR Unable to invoke factory method in class class org.apache.hadoop.hive.ql.log.HushableRandomAccessFileAppender for element HushableMutableRandomAccess. java.lang.reflect.InvocationTargetException
```
, отключите автоматический MapJoin. Это делается так: 

```
SET hive.auto.convert.join = false;
```

6. По умолчанию максимальное кол-во партиций в Hive 100, но в таблице Logs уникальных ключей больше 100 поэтому при запуске партиционирования появляется ошибка "Fatal error ocurred when node tried to created to many dynamic partitions". Увеличить кол-во партиций можно так:
```
SET hive.exec.max.dynamic.partitions=116;
SET hive.exec.max.dynamic.partitions.pernode=116;
```

